{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad5291e-19e7-40d1-9aae-31c417cd843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leosohrabi/venv-jupyter/bin/python\n",
      "3.13.3 (main, Apr  8 2025, 13:54:08) [Clang 16.0.0 (clang-1600.0.26.6)]\n",
      "Python 3.13.3\n",
      "Name: pyspark\n",
      "Version: 3.5.0\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import sys, site\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "!python -V\n",
    "%pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9812c3-2298-4f16-b723-81770bd78703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.0 in /Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages (3.5.0)\n",
      "Requirement already satisfied: pandas in /Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: pyarrow in /Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages (21.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages (from pyspark==3.5.0) (0.10.9.7)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.5.0 pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16ff31c8-db0a-4cac-97f8-9f9e6cd6eefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(\"PySpark version:\", pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963b697f-6a5e-4575-84b1-29559d9a3410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/leosohrabi/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/leosohrabi/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8253ccb5-bfbc-4edc-92e3-d4bd70728301;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 222ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8253ccb5-bfbc-4edc-92e3-d4bd70728301\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/4ms)\n",
      "25/10/07 13:47:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mac.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MastodonStreamDemo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1134bf770>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MastodonStreamDemo\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.7.3\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2745adc-5875-4f8f-9440-bbf9e949b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP = \"localhost:29092\"\n",
    "TOPIC = \"mastodon_stream\"\n",
    "\n",
    "JDBC_URL = \"jdbc:postgresql://localhost:5433/mastodon\"\n",
    "JDBC_PROPS = {\"user\":\"mastodon\",\"password\":\"mastodon\",\"driver\":\"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e4cbf8b-4d19-43a0-b6c7-1d867413bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, window, length, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"username\", StringType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "])\n",
    "\n",
    "kdf = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "    .option(\"subscribe\", TOPIC)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "base = (\n",
    "    kdf.selectExpr(\"CAST(value AS STRING) AS json\")\n",
    "       .select(from_json(col(\"json\"), schema).alias(\"data\"))\n",
    "       .select(\"data.*\")\n",
    "       .na.drop(subset=[\"username\",\"text\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51b92bde-2d05-41eb-a2c5-7a2c202c26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sink(batch_df, batch_id: int):\n",
    "    # 4.a Comptage par minute (fenêtrage tumbling 1 min)\n",
    "    toots_per_minute = (\n",
    "        base.groupBy(window(col(\"timestamp\").cast(\"timestamp\").alias(\"ts\"), \"1 minute\"))\n",
    "            if \"timestamp\" in base.columns\n",
    "            else base.withColumn(\"ts\", lit(None).cast(\"timestamp\"))\n",
    "                    .groupBy(window(col(\"ts\"), \"1 minute\"))\n",
    "    ).count().select(\n",
    "        lit(batch_id).alias(\"batch_id\"),\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"count\").alias(\"cnt\")\n",
    "    )\n",
    "\n",
    "    toots_per_minute.write.mode(\"append\").jdbc(JDBC_URL, \"streamed_toot_counts\", JDBC_PROPS)\n",
    "\n",
    "    # 4.b Longueur moyenne par utilisateur\n",
    "    avg_length_per_user = (\n",
    "        base.withColumn(\"length\", length(col(\"text\")))\n",
    "            .groupBy(\"username\")\n",
    "            .avg(\"length\")\n",
    "            .select(\n",
    "                lit(batch_id).alias(\"batch_id\"),\n",
    "                col(\"username\"),\n",
    "                col(\"avg(length)\").alias(\"avg_length\")\n",
    "            )\n",
    "    )\n",
    "    avg_length_per_user.write.mode(\"append\").jdbc(JDBC_URL, \"avg_toot_length_by_user\", JDBC_PROPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce3a6542-0ae4-4299-a89e-f2cc1a29c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP = \"localhost:29092\"   # ton Kafka docker exposé\n",
    "TOPIC = \"mastodon_stream\"\n",
    "\n",
    "JDBC_URL = \"jdbc:postgresql://localhost:5433/mastodon\"\n",
    "JDBC_PROPS = {\"user\": \"mastodon\", \"password\": \"mastodon\", \"driver\": \"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a75842ae-863e-49d4-be3c-f1f35453b607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table mastodon_posts prête\n"
     ]
    }
   ],
   "source": [
    "import psycopg2, textwrap\n",
    "sql = textwrap.dedent(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS mastodon_posts(\n",
    "  id SERIAL PRIMARY KEY,\n",
    "  username TEXT,\n",
    "  content  TEXT,\n",
    "  ts TIMESTAMPTZ DEFAULT NOW()\n",
    ");\n",
    "\"\"\")\n",
    "with psycopg2.connect(\"dbname=mastodon user=mastodon password=mastodon host=localhost port=5433\") as c:\n",
    "    with c.cursor() as cur: cur.execute(sql); c.commit()\n",
    "print(\"Table mastodon_posts prête\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f5c67eb-17f7-4486-9570-ae34e36fb84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages (2.9.10)\n",
      "Requirement already satisfied: sqlalchemy in /Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages (2.0.43)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages (from sqlalchemy) (4.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2-binary sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56d901df-ab99-47aa-b6f5-dec3e4a5cb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table mastodon_posts prête\n"
     ]
    }
   ],
   "source": [
    "import psycopg2, textwrap\n",
    "sql = textwrap.dedent(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS mastodon_posts(\n",
    "  id SERIAL PRIMARY KEY,\n",
    "  username TEXT,\n",
    "  content  TEXT,\n",
    "  ts TIMESTAMPTZ DEFAULT NOW()\n",
    ");\n",
    "\"\"\")\n",
    "with psycopg2.connect(\"dbname=mastodon user=mastodon password=mastodon host=localhost port=5433\") as c:\n",
    "    with c.cursor() as cur: cur.execute(sql); c.commit()\n",
    "print(\"Table mastodon_posts prête\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b67c3d39-a016-4075-8326-ddbb81b2d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/07 13:47:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mac.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MastodonStreamDemo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1134bf770>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MastoDemoLocal\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.7.3\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16cbfc24-0d9c-4895-bcbc-f7fa49c0f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, coalesce, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "KAFKA_BOOTSTRAP = \"localhost:29092\"\n",
    "TOPIC = \"mastodon_stream\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"username\", StringType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"content\", StringType()),\n",
    "])\n",
    "\n",
    "raw = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "    .option(\"subscribe\", TOPIC)\n",
    "    .option(\"startingOffsets\", \"latest\")   # parfait pour une démo\n",
    "    .load()\n",
    ")\n",
    "\n",
    "parsed = (\n",
    "    raw.selectExpr(\"CAST(value AS STRING) AS json\")\n",
    "       .select(from_json(col(\"json\"), schema).alias(\"data\"))\n",
    "       .select(\n",
    "           col(\"data.username\").alias(\"username\"),\n",
    "           coalesce(col(\"data.text\"), col(\"data.content\")).alias(\"content\"),\n",
    "           current_timestamp().alias(\"ts\")\n",
    "       )\n",
    "       .filter(col(\"username\").isNotNull() & col(\"content\").isNotNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bd38b92-07cc-4e38-9b4e-ec8cfd6095ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/07 13:47:28 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x114d92120>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JDBC_URL = \"jdbc:postgresql://localhost:5433/mastodon\"\n",
    "JDBC_PROPS = {\"user\": \"mastodon\", \"password\": \"mastodon\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "def sink_to_pg(batch_df, batch_id):\n",
    "    (batch_df\n",
    "        .write\n",
    "        .mode(\"append\")\n",
    "        .jdbc(JDBC_URL, \"mastodon_posts\", properties=JDBC_PROPS))\n",
    "\n",
    "query = (\n",
    "    parsed.writeStream\n",
    "          .foreachBatch(sink_to_pg)\n",
    "          .option(\"checkpointLocation\", \"/tmp/chkpt_masto_nb\")\n",
    "          .start()\n",
    ")\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ab73c28-871f-40b7-909c-de04d95a5133",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'exec'. Did you mean exec(...)? (650506559.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdocker exec -it kafka /opt/kafka/bin/kafka-console-producer.sh \\\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m Missing parentheses in call to 'exec'. Did you mean exec(...)?\n"
     ]
    }
   ],
   "source": [
    "docker exec -it kafka /opt/kafka/bin/kafka-console-producer.sh \\\n",
    "  --bootstrap-server kafka:9092 \\\n",
    "  --topic mastodon_stream\n",
    "# puis tape des lignes JSON :\n",
    ">{\"username\":\"leo\",\"text\":\"hello from notebook\"}\n",
    ">{\"username\":\"demo\",\"content\":\"stream to postgres\"}\n",
    ">{\"username\":\"leo\",\"text\":\"another toot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfdc543-b0c9-4923-9575-002ae7d37a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker exec -it kafka /opt/kafka/bin/kafka-console-producer.sh \\\n",
    "  --bootstrap-server kafka:9092 \\\n",
    "  --topic mastodon_stream\n",
    ">{\"username\":\"leo\",\"text\":\"hello from notebook\"}\n",
    ">{\"username\":\"demo\",\"content\":\"stream to postgres\"}\n",
    ">{\"username\":\"leo\",\"text\":\"another toot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2203e-6fbc-4dd7-9fdf-87aca4568c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, sqlalchemy as sa, time\n",
    "engine = sa.create_engine(\"postgresql+psycopg2://mastodon:mastodon@localhost:5433/mastodon\")\n",
    "for _ in range(5):\n",
    "    display(pd.read_sql(\"SELECT * FROM mastodon_posts ORDER BY id DESC LIMIT 10\", engine))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15643074-0cf7-4f3a-95b3-b485f23e89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark.stop() if 'spark' in globals() else None\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"masto-demo\")\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "            \"org.postgresql:postgresql:42.7.3\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41769d96-209e-4f70-84e9-13d448213dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"username\", T.StringType()),\n",
    "    T.StructField(\"text\",     T.StringType()),\n",
    "    T.StructField(\"content\",  T.StringType()),\n",
    "])\n",
    "\n",
    "kafka_df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:29092\")  # <- important !\n",
    "    .option(\"subscribe\", \"mastodon_stream\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "j = F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"j\")\n",
    "base = (kafka_df\n",
    "        .select(j)\n",
    "        .select(\n",
    "            F.col(\"j.username\").alias(\"username\"),\n",
    "            F.coalesce(F.col(\"j.text\"), F.col(\"j.content\")).alias(\"content\")\n",
    "        )\n",
    "        .filter(F.col(\"username\").isNotNull() & F.col(\"content\").isNotNull())\n",
    "       )\n",
    "\n",
    "def sink(batch_df, batch_id):\n",
    "    (batch_df\n",
    "     .write.format(\"jdbc\")\n",
    "     .option(\"url\", \"jdbc:postgresql://localhost:5433/mastodon\")\n",
    "     .option(\"dbtable\", \"mastodon_posts\")\n",
    "     .option(\"user\", \"mastodon\")\n",
    "     .option(\"password\", \"mastodon\")\n",
    "     .option(\"driver\", \"org.postgresql.Driver\")\n",
    "     .mode(\"append\")\n",
    "     .save())\n",
    "\n",
    "q = (base.writeStream\n",
    "     .foreachBatch(sink)\n",
    "     .option(\"checkpointLocation\", \"/tmp/chkpt_masto_nb\")\n",
    "     .start())\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a22ce8-830c-4cd2-9de1-a1b3effd6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP = \"localhost:29092\"   # <- important depuis le notebook\n",
    "TOPIC = \"mastodon_stream\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b7e3f-c9ae-40c0-ab8c-dadfaf22352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, get_json_object, coalesce\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"masto-notebook\")\n",
    "         .config(\"spark.ui.port\", \"0\")\n",
    "         .config(\"spark.jars.packages\",\n",
    "                 \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "                 \"org.postgresql:postgresql:42.7.3\")\n",
    "         .getOrCreate())\n",
    "\n",
    "raw = (spark.readStream\n",
    "       .format(\"kafka\")\n",
    "       .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "       .option(\"subscribe\", TOPIC)\n",
    "       .option(\"startingOffsets\", \"latest\")\n",
    "       .load())\n",
    "\n",
    "json = raw.selectExpr(\"CAST(value AS STRING) AS json\")\n",
    "\n",
    "df = (json\n",
    "      .select(\n",
    "          coalesce(get_json_object(\"json\", \"$.text\"),\n",
    "                   get_json_object(\"json\", \"$.content\")).alias(\"text\"),\n",
    "          get_json_object(\"json\", \"$.username\").alias(\"username\")\n",
    "      )\n",
    "      .where(col(\"text\").isNotNull() & col(\"username\").isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63c964-9e7f-44ba-ab37-3d9782940ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montre l’état et le dernier batch traité\n",
    "print(\"isActive:\", query.isActive)\n",
    "print(\"status:\", query.status)\n",
    "print(\"lastProgress:\", query.lastProgress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4555774-d99d-4fef-9bbd-426d159191af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/07 13:47:28 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import time, pandas as pd, sqlalchemy as sa\n",
    "\n",
    "engine = sa.create_engine(\"postgresql+psycopg2://mastodon:mastodon@localhost:5433/mastodon\")\n",
    "\n",
    "for _ in range(60):                 # ~60 secondes\n",
    "    df = pd.read_sql(\n",
    "        \"SELECT id, username, content, ts \"\n",
    "        \"FROM mastodon_posts ORDER BY id DESC LIMIT 10\",\n",
    "        engine\n",
    "    )\n",
    "    clear_output(wait=True)         # <-- efface l’affichage précédent\n",
    "    display(df)\n",
    "    time.sleep(1)                   # refresh chaque seconde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21a6263e-0f2c-44e2-8d17-41e1c592444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/07 13:47:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mac.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MastodonStreamDemo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1134bf770>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "  .appName(\"MastodonStreamProcessing\")\n",
    "  .config(\"spark.jars.packages\",\n",
    "          \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "          \"org.postgresql:postgresql:42.7.3\")\n",
    "  .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af69af66-851b-40c3-ba1c-f20421cf75c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[FIELD_NOT_FOUND] No such struct field `*` in `username`, `text`, `content`, `ts`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     16\u001b[39m raw = (spark.readStream.format(\u001b[33m\"\u001b[39m\u001b[33mkafka\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mkafka.bootstrap.servers\u001b[39m\u001b[33m\"\u001b[39m, KAFKA_BOOTSTRAP)\n\u001b[32m     18\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33msubscribe\u001b[39m\u001b[33m\"\u001b[39m, TOPIC)\n\u001b[32m     19\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mstartingOffsets\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlatest\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m     .load())\n\u001b[32m     22\u001b[39m json = from_json(col(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m), schema)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m base = (\u001b[43mraw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     .withColumn(\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m, coalesce(col(\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m), col(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m     26\u001b[39m     .withColumn(\u001b[33m\"\u001b[39m\u001b[33mts\u001b[39m\u001b[33m\"\u001b[39m, to_timestamp(\u001b[33m\"\u001b[39m\u001b[33mts\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msink\u001b[39m(df, batch_id):\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df.rdd.isEmpty(): \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv-jupyter/lib/python3.13/site-packages/pyspark/sql/dataframe.py:3223\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m   3178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   3179\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   3180\u001b[39m \n\u001b[32m   3181\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3221\u001b[39m \u001b[33;03m    +-----+---+\u001b[39;00m\n\u001b[32m   3222\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3223\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv-jupyter/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv-jupyter/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [FIELD_NOT_FOUND] No such struct field `*` in `username`, `text`, `content`, `ts`."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, from_json, to_timestamp, current_timestamp, coalesce\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "KAFKA_BOOTSTRAP = \"localhost:9092\"\n",
    "TOPIC = \"mastodon_stream\"\n",
    "JDBC_URL = \"jdbc:postgresql://localhost:5433/mastodon\"\n",
    "JDBC_PROPS = {\"user\": \"mastodon\", \"password\": \"mastodon\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"username\", StringType()),\n",
    "    StructField(\"text\",     StringType()),\n",
    "    StructField(\"content\",  StringType()),\n",
    "    StructField(\"ts\",       StringType())\n",
    "])\n",
    "\n",
    "raw = (spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "    .option(\"subscribe\", TOPIC)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load())\n",
    "\n",
    "json = from_json(col(\"value\").cast(\"string\"), schema)\n",
    "\n",
    "base = (raw.select(json[\"*\"])\n",
    "    .withColumn(\"content\", coalesce(col(\"content\"), col(\"text\")))\n",
    "    .withColumn(\"ts\", to_timestamp(\"ts\")))\n",
    "\n",
    "def sink(df, batch_id):\n",
    "    if df.rdd.isEmpty(): return\n",
    "    (df.withColumn(\"ts\", coalesce(col(\"ts\"), current_timestamp()))\n",
    "       .select(\"username\", \"content\", \"ts\")\n",
    "       .write.mode(\"append\").jdbc(JDBC_URL, \"mastodon_posts\", properties=JDBC_PROPS))\n",
    "\n",
    "query = (base.writeStream\n",
    "    .foreachBatch(sink)\n",
    "    .option(\"checkpointLocation\", \"/tmp/chkpt_masto\")\n",
    "    .start())\n",
    "query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193b4c3-ab81-4aef-95cb-e644b1c9a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import pandas as pd, sqlalchemy as sa, time\n",
    "\n",
    "engine = sa.create_engine(\"postgresql+psycopg2://mastodon:mastodon@localhost:5433/mastodon\")\n",
    "\n",
    "for _ in range(60):  # 60 secondes\n",
    "    clear_output(wait=True)\n",
    "    df = pd.read_sql(\"SELECT id, username, content, ts FROM mastodon_posts ORDER BY id DESC LIMIT 10\", engine)\n",
    "    display(df)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e167e6a-c890-4c47-9769-e7bbc49060a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mac.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MastodonStreamDemo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1134bf770>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "  .appName(\"MastodonStreamProcessing\")\n",
    "  .config(\"spark.jars.packages\",\n",
    "          \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "          \"org.postgresql:postgresql:42.7.3\")\n",
    "  .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c41bb8d-e407-4be1-b278-ebe317ebdeb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[FIELD_NOT_FOUND] No such struct field `*` in `username`, `text`, `content`, `ts`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     16\u001b[39m raw = (spark.readStream.format(\u001b[33m\"\u001b[39m\u001b[33mkafka\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mkafka.bootstrap.servers\u001b[39m\u001b[33m\"\u001b[39m, KAFKA_BOOTSTRAP)\n\u001b[32m     18\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33msubscribe\u001b[39m\u001b[33m\"\u001b[39m, TOPIC)\n\u001b[32m     19\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mstartingOffsets\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlatest\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m     .load())\n\u001b[32m     22\u001b[39m json = from_json(col(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m), schema)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m base = (\u001b[43mraw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     .withColumn(\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m, coalesce(col(\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m), col(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m     26\u001b[39m     .withColumn(\u001b[33m\"\u001b[39m\u001b[33mts\u001b[39m\u001b[33m\"\u001b[39m, to_timestamp(\u001b[33m\"\u001b[39m\u001b[33mts\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msink\u001b[39m(df, batch_id):\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df.rdd.isEmpty(): \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv-jupyter/lib/python3.13/site-packages/pyspark/sql/dataframe.py:3223\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m   3178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   3179\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   3180\u001b[39m \n\u001b[32m   3181\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3221\u001b[39m \u001b[33;03m    +-----+---+\u001b[39;00m\n\u001b[32m   3222\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3223\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv-jupyter/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv-jupyter/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [FIELD_NOT_FOUND] No such struct field `*` in `username`, `text`, `content`, `ts`."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, from_json, to_timestamp, current_timestamp, coalesce\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "KAFKA_BOOTSTRAP = \"localhost:9092\"\n",
    "TOPIC = \"mastodon_stream\"\n",
    "JDBC_URL = \"jdbc:postgresql://localhost:5433/mastodon\"\n",
    "JDBC_PROPS = {\"user\": \"mastodon\", \"password\": \"mastodon\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"username\", StringType()),\n",
    "    StructField(\"text\",     StringType()),\n",
    "    StructField(\"content\",  StringType()),\n",
    "    StructField(\"ts\",       StringType())\n",
    "])\n",
    "\n",
    "raw = (spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "    .option(\"subscribe\", TOPIC)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load())\n",
    "\n",
    "json = from_json(col(\"value\").cast(\"string\"), schema)\n",
    "\n",
    "base = (raw.select(json[\"*\"])\n",
    "    .withColumn(\"content\", coalesce(col(\"content\"), col(\"text\")))\n",
    "    .withColumn(\"ts\", to_timestamp(\"ts\")))\n",
    "\n",
    "def sink(df, batch_id):\n",
    "    if df.rdd.isEmpty(): return\n",
    "    (df.withColumn(\"ts\", coalesce(col(\"ts\"), current_timestamp()))\n",
    "       .select(\"username\", \"content\", \"ts\")\n",
    "       .write.mode(\"append\").jdbc(JDBC_URL, \"mastodon_posts\", properties=JDBC_PROPS))\n",
    "\n",
    "query = (base.writeStream\n",
    "    .foreachBatch(sink)\n",
    "    .option(\"checkpointLocation\", \"/tmp/chkpt_masto\")\n",
    "    .start())\n",
    "query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63564e15-4879-4f9a-875a-16defa1da966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>content</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Potter</td>\n",
       "      <td>hello Sirius</td>\n",
       "      <td>2025-10-07 10:02:40.046819+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>demo</td>\n",
       "      <td>new toot from kafka</td>\n",
       "      <td>2025-10-07 09:59:53.613210+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>leo</td>\n",
       "      <td>another toot</td>\n",
       "      <td>2025-10-07 09:59:37.772464+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>demo</td>\n",
       "      <td>stream to postgres</td>\n",
       "      <td>2025-10-07 09:58:08.418246+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>leo</td>\n",
       "      <td>hello from notebook</td>\n",
       "      <td>2025-10-07 09:58:05.265591+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id username              content                               ts\n",
       "0   5   Potter         hello Sirius 2025-10-07 10:02:40.046819+00:00\n",
       "1   4     demo  new toot from kafka 2025-10-07 09:59:53.613210+00:00\n",
       "2   3      leo         another toot 2025-10-07 09:59:37.772464+00:00\n",
       "3   2     demo   stream to postgres 2025-10-07 09:58:08.418246+00:00\n",
       "4   1      leo  hello from notebook 2025-10-07 09:58:05.265591+00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import pandas as pd, sqlalchemy as sa, time\n",
    "\n",
    "engine = sa.create_engine(\"postgresql+psycopg2://mastodon:mastodon@localhost:5433/mastodon\")\n",
    "\n",
    "for _ in range(60):  # 60 secondes\n",
    "    clear_output(wait=True)\n",
    "    df = pd.read_sql(\"SELECT id, username, content, ts FROM mastodon_posts ORDER BY id DESC LIMIT 10\", engine)\n",
    "    display(df)\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c831a-11d8-40e7-8db4-5230cb07729b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>content</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Potter</td>\n",
       "      <td>hello Sirius</td>\n",
       "      <td>2025-10-07 10:02:40.046819+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>demo</td>\n",
       "      <td>new toot from kafka</td>\n",
       "      <td>2025-10-07 09:59:53.613210+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>leo</td>\n",
       "      <td>another toot</td>\n",
       "      <td>2025-10-07 09:59:37.772464+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>demo</td>\n",
       "      <td>stream to postgres</td>\n",
       "      <td>2025-10-07 09:58:08.418246+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>leo</td>\n",
       "      <td>hello from notebook</td>\n",
       "      <td>2025-10-07 09:58:05.265591+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id username              content                               ts\n",
       "0   5   Potter         hello Sirius 2025-10-07 10:02:40.046819+00:00\n",
       "1   4     demo  new toot from kafka 2025-10-07 09:59:53.613210+00:00\n",
       "2   3      leo         another toot 2025-10-07 09:59:37.772464+00:00\n",
       "3   2     demo   stream to postgres 2025-10-07 09:58:08.418246+00:00\n",
       "4   1      leo  hello from notebook 2025-10-07 09:58:05.265591+00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import pandas as pd, sqlalchemy as sa, time\n",
    "\n",
    "engine = sa.create_engine(\"postgresql+psycopg2://mastodon:mastodon@localhost:5433/mastodon\")\n",
    "\n",
    "for _ in range(60):  # 60 secondes d’affichage\n",
    "    clear_output(wait=True)\n",
    "    df = pd.read_sql(\n",
    "        \"SELECT id, username, content, ts FROM mastodon_posts ORDER BY id DESC LIMIT 10\",\n",
    "        engine\n",
    "    )\n",
    "    display(df)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af53997b-f532-4e7d-b439-a39a3b353f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "OK - libs chargées\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pyspark==3.5.0 pandas pyarrow psycopg2-binary sqlalchemy\n",
    "import pyspark, pandas, psycopg2, sqlalchemy\n",
    "print(\"OK - libs chargées\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43669bf3-5f60-4787-91ec-b67a5d358e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/leosohrabi/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/leosohrabi/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7503855d-6068-4683-9717-4cbc010dd909;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 250ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7503855d-6068-4683-9717-4cbc010dd909\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/5ms)\n",
      "25/10/10 13:51:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mac.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MastodonStreamDemo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x116ded550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/10 13:51:40 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MastodonStreamDemo\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.7.3\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cbfeeb4-3369-481d-857f-f40d2fe8da5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables OK\n"
     ]
    }
   ],
   "source": [
    "import psycopg2, textwrap\n",
    "\n",
    "DDL = textwrap.dedent(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS mastodon_posts(\n",
    "  id SERIAL PRIMARY KEY,\n",
    "  username TEXT,\n",
    "  content  TEXT,\n",
    "  ts TIMESTAMPTZ DEFAULT NOW()\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS streamed_toot_counts (\n",
    "  batch_id INT, window_start TIMESTAMPTZ, window_end TIMESTAMPTZ, cnt BIGINT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS avg_toot_length_by_user (\n",
    "  batch_id INT, username TEXT, avg_length DOUBLE PRECISION\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "with psycopg2.connect(\"dbname=mastodon user=mastodon password=mastodon host=localhost port=5433\") as c:\n",
    "    with c.cursor() as cur:\n",
    "        cur.execute(DDL)\n",
    "        c.commit()\n",
    "print(\"Tables OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e57f64b-5032-4f84-a460-4364233b7e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/10 13:53:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Initializing sources',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/10 13:53:32 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/10/10 13:53:33 ERROR MicroBatchExecution: Query [id = 49e8eace-dfc4-4c8e-b9a0-3741efe8291d, runId = 275625f4-a02d-4ba5-a369-0a35b2c6edb6] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/9f/rq0cd3490qj68nrrscsb2b6w0000gn/T/ipykernel_21641/4015849868.py\", line 34, in sink\n",
      "    .jdbc(JDBC_URL, \"mastodon_posts\", properties=JDBC_PROPS))\n",
      "     ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages/pyspark/sql/readwriter.py\", line 1984, in jdbc\n",
      "    self.mode(mode)._jwrite.jdbc(url, table, jprop)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "        answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/Users/leosohrabi/venv-jupyter/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Column text not found in schema Some(StructType(StructField(id,IntegerType,false),StructField(username,StringType,true),StructField(content,StringType,true),StructField(ts,TimestampType,true))).\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, from_json, when, length, window, lit, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "KAFKA_BOOTSTRAP = \"localhost:29092\"\n",
    "TOPIC = \"mastodon_stream\"\n",
    "\n",
    "JDBC_URL = \"jdbc:postgresql://localhost:5433/mastodon\"\n",
    "JDBC_PROPS = {\"user\": \"mastodon\", \"password\": \"mastodon\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# --- Définition du schéma attendu depuis Kafka\n",
    "schema = StructType([\n",
    "    StructField(\"username\", StringType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "])\n",
    "\n",
    "# --- Lecture du flux Kafka\n",
    "kdf = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "    .option(\"subscribe\", TOPIC)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load())\n",
    "\n",
    "base = (kdf.selectExpr(\"CAST(value AS STRING) AS json\")\n",
    "          .select(from_json(col(\"json\"), schema).alias(\"data\"))\n",
    "          .select(\"data.*\")\n",
    "          .na.drop(subset=[\"username\", \"text\"])\n",
    "          .withColumn(\"ts\", current_timestamp()))\n",
    "\n",
    "# --- Fonction de sink pour écrire dans Postgres\n",
    "def sink(batch_df, batch_id):\n",
    "    (batch_df.select(\"username\", \"text\", \"ts\")\n",
    "        .write.mode(\"append\")\n",
    "        .jdbc(JDBC_URL, \"mastodon_posts\", properties=JDBC_PROPS))\n",
    "\n",
    "checkpoint_dir = \"/tmp/chkpt_masto_run_1\"\n",
    "\n",
    "query = (base.writeStream\n",
    "  .foreachBatch(sink)\n",
    "  .option(\"checkpointLocation\", checkpoint_dir)\n",
    "  .start())\n",
    "\n",
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9e094ad-c98f-4ec1-99bb-76de98cf8afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/10 13:58:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Initializing sources',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/10 13:58:56 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1) Stopper un éventuel stream en cours\n",
    "try:\n",
    "    if 'query' in globals() and query.isActive:\n",
    "        query.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from pyspark.sql.functions import col, from_json, when, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "KAFKA_BOOTSTRAP = \"localhost:29092\"\n",
    "TOPIC = \"mastodon_stream\"\n",
    "\n",
    "JDBC_URL = \"jdbc:postgresql://localhost:5433/mastodon\"\n",
    "JDBC_PROPS = {\"user\": \"mastodon\", \"password\": \"mastodon\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# Schéma qui accepte à la fois 'text' et 'content'\n",
    "schema = StructType([\n",
    "    StructField(\"username\", StringType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"content\", StringType()),\n",
    "])\n",
    "\n",
    "# Lecture Kafka\n",
    "kdf = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "    .option(\"subscribe\", TOPIC)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load())\n",
    "\n",
    "# Parsing + normalisation du champ contenu -> 'content'\n",
    "base = (kdf.selectExpr(\"CAST(value AS STRING) AS json\")\n",
    "          .select(from_json(col(\"json\"), schema).alias(\"j\"))\n",
    "          .select(\n",
    "              col(\"j.username\").alias(\"username\"),\n",
    "              when(col(\"j.text\").isNotNull(), col(\"j.text\"))\n",
    "                .otherwise(col(\"j.content\")).alias(\"content\")\n",
    "          )\n",
    "          .na.drop(subset=[\"username\",\"content\"])\n",
    "          .withColumn(\"ts\", current_timestamp())\n",
    ")\n",
    "\n",
    "# Sink : écrit 'username, content, ts' dans mastodon_posts\n",
    "def sink(batch_df, batch_id):\n",
    "    (batch_df.select(\"username\", \"content\", \"ts\")\n",
    "        .write.mode(\"append\")\n",
    "        .jdbc(JDBC_URL, \"mastodon_posts\", properties=JDBC_PROPS))\n",
    "\n",
    "checkpoint_dir = \"/tmp/chkpt_masto_run_fix1\"   # nouveau dossier pour repartir clean\n",
    "\n",
    "query = (base.writeStream\n",
    "  .foreachBatch(sink)\n",
    "  .option(\"checkpointLocation\", checkpoint_dir)\n",
    "  .start())\n",
    "\n",
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cbf8c14-3067-4633-9e94-d81f3f9c0f87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>content</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>TestDuJour</td>\n",
       "      <td>hello from kafka</td>\n",
       "      <td>2025-10-10 12:02:47.386000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>demo</td>\n",
       "      <td>works with content too</td>\n",
       "      <td>2025-10-10 12:01:52.915000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>leo</td>\n",
       "      <td>hello from kafka</td>\n",
       "      <td>2025-10-10 12:01:50.716000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Potter</td>\n",
       "      <td>hello Sirius</td>\n",
       "      <td>2025-10-07 10:02:40.046819+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>demo</td>\n",
       "      <td>new toot from kafka</td>\n",
       "      <td>2025-10-07 09:59:53.613210+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>leo</td>\n",
       "      <td>another toot</td>\n",
       "      <td>2025-10-07 09:59:37.772464+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>demo</td>\n",
       "      <td>stream to postgres</td>\n",
       "      <td>2025-10-07 09:58:08.418246+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>leo</td>\n",
       "      <td>hello from notebook</td>\n",
       "      <td>2025-10-07 09:58:05.265591+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    username                 content                               ts\n",
       "0   8  TestDuJour        hello from kafka 2025-10-10 12:02:47.386000+00:00\n",
       "1   7        demo  works with content too 2025-10-10 12:01:52.915000+00:00\n",
       "2   6         leo        hello from kafka 2025-10-10 12:01:50.716000+00:00\n",
       "3   5      Potter            hello Sirius 2025-10-07 10:02:40.046819+00:00\n",
       "4   4        demo     new toot from kafka 2025-10-07 09:59:53.613210+00:00\n",
       "5   3         leo            another toot 2025-10-07 09:59:37.772464+00:00\n",
       "6   2        demo      stream to postgres 2025-10-07 09:58:08.418246+00:00\n",
       "7   1         leo     hello from notebook 2025-10-07 09:58:05.265591+00:00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, sqlalchemy as sa\n",
    "engine = sa.create_engine(\"postgresql+psycopg2://mastodon:mastodon@localhost:5433/mastodon\")\n",
    "pd.read_sql(\"SELECT id, username, content, ts FROM mastodon_posts ORDER BY id DESC LIMIT 10\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510e70b-3fd9-4df3-84fa-bfff4cbbb946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>content</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>Bat</td>\n",
       "      <td>works with content too</td>\n",
       "      <td>2025-10-10 12:06:01.634000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>TestDuJour</td>\n",
       "      <td>hello from kafka</td>\n",
       "      <td>2025-10-10 12:02:47.386000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>demo</td>\n",
       "      <td>works with content too</td>\n",
       "      <td>2025-10-10 12:01:52.915000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>leo</td>\n",
       "      <td>hello from kafka</td>\n",
       "      <td>2025-10-10 12:01:50.716000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Potter</td>\n",
       "      <td>hello Sirius</td>\n",
       "      <td>2025-10-07 10:02:40.046819+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>demo</td>\n",
       "      <td>new toot from kafka</td>\n",
       "      <td>2025-10-07 09:59:53.613210+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>leo</td>\n",
       "      <td>another toot</td>\n",
       "      <td>2025-10-07 09:59:37.772464+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>demo</td>\n",
       "      <td>stream to postgres</td>\n",
       "      <td>2025-10-07 09:58:08.418246+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>leo</td>\n",
       "      <td>hello from notebook</td>\n",
       "      <td>2025-10-07 09:58:05.265591+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    username                 content                               ts\n",
       "0   9         Bat  works with content too 2025-10-10 12:06:01.634000+00:00\n",
       "1   8  TestDuJour        hello from kafka 2025-10-10 12:02:47.386000+00:00\n",
       "2   7        demo  works with content too 2025-10-10 12:01:52.915000+00:00\n",
       "3   6         leo        hello from kafka 2025-10-10 12:01:50.716000+00:00\n",
       "4   5      Potter            hello Sirius 2025-10-07 10:02:40.046819+00:00\n",
       "5   4        demo     new toot from kafka 2025-10-07 09:59:53.613210+00:00\n",
       "6   3         leo            another toot 2025-10-07 09:59:37.772464+00:00\n",
       "7   2        demo      stream to postgres 2025-10-07 09:58:08.418246+00:00\n",
       "8   1         leo     hello from notebook 2025-10-07 09:58:05.265591+00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import pandas as pd, sqlalchemy as sa, time\n",
    "\n",
    "engine = sa.create_engine(\"postgresql+psycopg2://mastodon:mastodon@localhost:5433/mastodon\")\n",
    "\n",
    "# Affiche les dernières lignes en continu pendant 60 secondes (~1 minute)\n",
    "for _ in range(60):\n",
    "    clear_output(wait=True)\n",
    "    df = pd.read_sql(\n",
    "        \"SELECT id, username, content, ts FROM mastodon_posts ORDER BY id DESC LIMIT 10\",\n",
    "        engine\n",
    "    )\n",
    "    display(df)\n",
    "    time.sleep(2)  # rafraîchit toutes les 2 secondes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea3b64-28f3-4f7b-9fb0-8ab47ebb0d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (venv)",
   "language": "python",
   "name": "python3.13-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
